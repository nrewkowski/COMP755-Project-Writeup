\section{Introduction}

%%%%% Problem/Applications/Challenge
Over the last few years, virtual reality (VR) and augmented reality (AR)
technologies have demonstrated the potential to effect the way we live and work.
Understanding user behavior in VR/AR is very important for both creating
content and adapting to different user models. In this paper, we use
knowledge of where the user looks to detect objects and
understand various user behaviors such as
\begin{inlist}
\item looking at an object,
\item being interested in the object (e.g., predicted trajectory), or
\item
    paying attention to a certain detail (e.g., a training surgeon carefully
    assembling the surgical bench).
\end{inlist}

%%%%% Background of object detection + our work
Currently, object detection and eye tracking are both well-understood problems.
State-of-the-art methods unanimously employ variants of convolutional neural
networks (CNNs). A number of object detectors~\cite{redmon2016you, liu2016ssd,
lin2017focal, girshick2014rich, girshick2015fast, ren2015faster, he2017mask}
achieve incredible results on large-scale benchmarks. These networks are
designed to recognize low-resolution data in a large search space, relying on a
deep neural network with a high memory capacity requirement. In order to achieve
a more accurate and performant version on VR/AR platforms, we propose a novel
gaze-guided object detection method that first tracks gaze with a built-in
eye-tracker in the MagicLeap One and then segments and detects the focused object with a
smaller neural network.

To decrease time cost as well as collection difficulty, we generated a huge
synthetic dataset including both eye images and scene images in an AR
environment. Game engines, which are capable of aligning real and virtual worlds
with markers, can provide the ground truth of focused point without the need for
manual labelling. From this, we can move on to more complex scenarios and use
such labelling for tasks such as generation of behavioral graphs or other
modern trends. Some use cases that motivated this project include:

\begin{description}
    \item[Adaptive Impaired-user Keyboards]
        If some symbols on a virtual keyboard are harder to use than others, or
        are used frequently together, re-arranging the layout to correspond to
        the user's needs may improve typing experience. Since some of these are
        joystick-input based and therefore further slow user's typing rate, the
        user's gaze can identify what symbol they are typing and aid the user in
        selecting it. Some users with mobility impairments may also want to
        select an item by blinking, speaking, or other means. Improvements in
        this area (such as via faster object detection based on gaze, as in this
        paper) could prove important because different disabilities require
        different customized keyboards for comfortable typing.
    \item[Surgical Training]
        Surgical students learn from the difference between the steps they take
        and the correct steps. For example, if a student's job is to organize
        and clean tools for a surgery, we can use eye-tracking and logs from the
        real-time application to generate a behavior graph (push-down
        automata-style) and determine if they are missing important steps or
        glossing over details.
    \item[Dynamic Robot AI]
        AI assisted by object recognition and behavioral understanding can
        better determine how to respond to user actions, such as in the case of
        a robot guide dog pulling users towards their object of interest. For
        example, the dog should realize when a VR user is too close to a wall
        and try to guide them away.
\end{description}

%%%%% Contribution
Our main contributions can be summarized as follows:

\begin{itemize}  [noitemsep]
    \item
        We automatically generate a ground truth object of interest with a
        real-time AR simulation in a game engine using raycasting
        (using Python, C++, and the Unreal Engine).
    \item
        We develop a system to generate large, physically-based synthetic image datasets with labels within minutes for future eye-tracking research using currently or formerly free Unreal 4 assets.
    \item
        We introduce a more efficient method of integrating object detection
        with eye tracking that reduces the amount of computation.
    \item
        We evaluate both detection methods with and without eye tracking and
        demonstrate that our method compares favorably against detection methods
        without eye tracking.
\end{itemize}

Code for synthetic generation can be found
\textbf{\href{https://github.com/nrewkowski/COMP755FinalProject}{\textcolor{cyan}{here}}},
and code for evaluation can be found
\textbf{\href{https://github.com/FloralZhao/Comp755Project}{\textcolor{cyan}{here}}}.
