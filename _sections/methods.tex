\section{Methods}

The system used for this project is a pipeline consisting of
\begin{inlist}
\item synthetic image data collection,
\item automatic creation of ground truth labels,
\item intermediate image processing,
\item training, and
\item image recognition during a real-time application.
\end{inlist}

\subsection{Synthetic Data}\label{S:synthetic-generation}

While there are existing datasets of eyes for eye-tracking research, their
purpose is significantly different from ours and generally only include labels
such as eye rotation; to our knowledge, such a dataset does not exist which
labels the object of interest. Thus, for this project, we must generate
synthetic data ourselves. We require synthetic data as we would not be able to
easily record real data in this scenario given both the tediousness of doing so
and our quarantine.

Much like how \emph{UnityEyes} renders eye images very quickly using a
real-time game engine, we used Unreal Engine 4 (UE4) to render
\begin{inlist}
\item camera images of the eyes,
\item
    camera images from the front-facing camera approximate the user's entire
    field of view, and
\item ground truth labels for what object the user is looking at.
\end{inlist}
We chose UE4 for a few reasons:

\begin{itemize}
    \item
        It has a strong physically-based rendering (PBR) system allowing for
        much more realistic materials to be used than Unity and with better
        performance (in terms of time-to-compile and render)
    \item
        It natively supports ray-tracing and cone-tracing allowing for more
        accurate eye reflections (which heavily impact eye-tracking quality in
        real life) and more accurate environmental reflections in general
    \item
        It supports C++, allowing interaction with image recognition libraries
        such as NVIDIA's
    \item
        NVIDIA, a major name in deep neural networks in general, has much
        stronger experimental features for UE4 such as CUDA, geometry
        interaction, etc.
\end{itemize}

We used free, high-quality architectural visualization (archviz) scenes for UE4
such as ``Realistic Rendering'' which contains many environmental objects with
clear, intuitive boundaries for the rendering of the user viewpoint and
labelling of focused objects. The objects in this scene include vases, books,
furniture, paintings, and plants. They are labelled as such in the default
scene. For the eye-facing camera images, we used Adobe Mixamo's character
collection to handle different eye and body shapes and materials. These
characters most importantly include blendshapes, which allowed us to make them
blink naturally during data collection, more wholly resembling real data. The
limitation of these characters is that their materials are not highly detailed
and some detailed facial features like accurate wrinkles on the eyelids are
missing. Examples of what the generated images look like can be found in the
supplemental material.

\subsection{Automatically Generating Ground Truth Labels}

An important feature of real-time game engines is native and efficient support
for raycasts. Raycasts are functions that shoot rays from one point to another,
returning
\begin{inlist}
\item all objects hit by the ray as well as hit location,
\item hit normal,
\item hit polygon,
\item etc.
\end{inlist}
These allow for the automatic deduction of what a camera is looking at. In the
case of our eye-tracking setup, the MagicLeap One gave the 3D point that the
users' eyes converge on; thus, we raycasted from the front-facing camera
location to this point of convergence and returned the first object hit to
estimate the object being looked at by the user.

In order to generate the required dataset, we defined a 3D ``scanpath''
describing the trajectory of the user's gaze that spans all of the objects in
the scene and eventually causing the character's body to turn 360 degrees. For
each Mixamo character, their eyes followed the scanpath and for each frame (at
60fps), we generated the eye-facing images, the front-facing camera image, and
the label for what the raycast hit during that frame. It currently took 2
minutes for the character to make the full rotation as this scene is relatively
small; we plan on adding more complexity later. For future data collection, we
may randomly select objects to look at in the 3D environment to avoid the manual
placement of scanpaths which can be a limiting factor in larger scenes.

\subsection{Intermediate Image Processing}

This intermediate step occurred between the synthetic data generation and training
because networks like Inception V3 were not intended to handle more than 1 image
per label, e.g., they would not understand the relationship between the eye
camera and front-facing camera images. The goals here were to
\begin{inlist}
\item simplify inputs to the image recognition network and to
\item incorporate information learned from the user's gaze.
\end{inlist}
We explored 2 techniques (cropping and Gaussian blur) to do so.

\paragraph{Cropping}

One way to limit the complexity of input images and simultaneously incorporate
gaze information is to crop the eye-camera image to the boundaries of the object
being gazed at. This simplification allows for a lower-resolution image with a
single primary object (and label), meeting our two objectives. We hypothesized
that networks such as Inception would classify equally accurately or better and
do so more quickly or with less computational complexity on such cropped inputs
than on traditional, more complex inputs without gaze information because of the
reduced complexity and smaller number of objects per image.

All input images must still maintain the same input dimensions, so this cropping
must be done carefully with that in mind.

% more details on how we cropped here
For each original image, we created two cropped images; one has dimensions
of \(\frac{1}{2}\) of the original, while the other has dimensions
\(\frac{1}{3}\).

\paragraph{Gaussian Blur}

Another technique to constrain the inputs and incorporate gaze information is to
apply a Gaussian blur to the image. This technique simulates overlaying a
2-dimensional Gaussian distribution on the image, centered on the focal point of
the user's gaze. Variance is computed as a function of the object's size.
Regions of low probability mass are blurred heavily, while regions of high
probability mass are blurred correspondingly less.\footnote{This is similar to
how foveated depth-of-field works. Foveated depth-of-field is a more recent
method in AR/VR in which depth-of-field is applied automatically based on
eye-tracking.} This has the net effect of blurring the objects the user is not
looking at and leaving the target object in focus. The resulting image provides
a mostly clear picture of the target object. We hypothesized that networks such
as Inception would classify equally accurately or better and do so more quickly
on such blurred inputs than on traditional, more complex inputs without gaze
information because any other objects in the image become less recognizable,
while the primary objects stays in focus.

% more details and any math/equations here
% TODO what are these heights relative to? The object or the image?
For each original image, we created two blurred images. Each uses a ``circular''
2-dimensional Gaussian (i.e., the 2 dimensions are independent and \(\Sigma =
\sigma I\) for some scaling factor \(\sigma\)). One uses \(\sigma =
\frac{\text{height}}{2}\), while the other uses \(\frac{\text{height}}{3}\).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% If we need information on foveation, we can do it here and \cite{Wang_2016}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Training \& Recognition}

The training and recognition step involved using the popular Inception V3
network to recognize the primary object in the images given to the network.
Since the game engine generated the labels automatically, the ground truth
labels were retrieved easily and were given to the network as with any other use
of this network. Beyond that, this step was fairly standard image recognition and
details of Inception V3 are better found in that network's API\@.
