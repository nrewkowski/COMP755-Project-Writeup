\section{Conclusion}

% accomplishments?
% learned?
% extensions/contributions to field?
\subsection{Future work}
% directions to explore

\subsubsection{Testing with a Real-time Application}
One direction to explore would be to verify current results in real-time
applications, where computational complexity and speed matter due to the
constraints of the applications. We mentioned several possible applications in
the introduction, including
\begin{inlist}
\item adaptive impaired-user keyboards,
\item surgical training, and
\item dynamic robot AI\@.
\end{inlist}
Ultimately, these applications will also benefit from behavioral graph analysis
or generation, another motivating factor of our work.

During a real-time MagicLeap application, we should only require the images from
the front-facing camera and the 3D focal point generated by the MagicLeap API, which can be processed with the same pipeline described above. The
3D scene from the game engine would no longer be required, and it is even
possible that a different, less bulky HMD (besides the MagicLeap) could be used
as long as it contains the necessary cameras (e.g., embedded Raspberry Pi
cameras). 

Finding the ground truth would be more difficult if testing performance of this camera data. In order to help validate the ability of the system to work on real-world camera
images, and in order to get the correct labels for the objects of interest, it
would be easiest to align some virtual objects to the real objects in order to
use raycasts for automatic labelling. This alignment can be done automatically
with image marker-tracking libraries that work natively with the MagicLeap and
Hololens such as OpenCV and Vuforia.

Given the time frame and limitations of quarantine, we were not able to test the system on the MagicLeap One, and doing so would not be wise until improving dataset performance. However, the application is very lucrative for future research as AR moves into the mainstream.

\subsection{Comparison of Visual Effects}
We did not have the chance to compare the effect of various visual styles and effects throughout the different datasets. It would be an interesting next step to do pairwise comparisons and/or apply stylistic filters to the images before prediction. The effect of particle effects is somewhat harder to measure, but may be feasible but comparing the performance of scenes like Warehouse and Ocean when all moving objects such as particles/fish are disabled completely.
\subsection{Randomized Objects}
While we do generate the \textbf{\_extraprops} datasets, more work can be done to verify that this augmentation is actually helping with the classification, such as in \cite{chabra2020deep}. In particular, we should try to retrain Inception V3 using these augmented datasets to see if there is an improvement.
\subsection{behavior graphs}
game engines useful for this
\subsection{temporal predictions}