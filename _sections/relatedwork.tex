\section{Related Work}
%nick knows nothing about this. nick doesn't pay attention in class either so
%doesn't know where to look. also neurips, iccv, cvpr, etc. are filled with fake
%papers so nick's not confident in their results anyway

\paragraph{Eye tracking}

Eye tracking is defined as the process of identifying the line of sight for each
eye of a human user at a single instant~\cite{kim2019nvgaze}. In this work, we
focus on vision-based eye tracking, which is mainly comprised of three methods:
feature-based, model-based and appearance-based. Feature-based eye tracking
localizes key eye features, such as iris center and eye corner, and map to
corresponding gaze coordinates on the screen~\cite{sesma2012evaluation,
torricelli2008neural}. Model-based methods~\cite{wood20163d, wang2017real}
estimates eye gaze by using a geometrical model of the human eye, which simulate
the structure and function of human vision system. State-of-the-art
appearance-based methods are occupied by artificial neural networks. A huge
collection of eye images are directly input into a neural network and trained to
output the final gaze~\cite{zhang2015appearance, schneider2014manifold,
sugano2014learning}. A hybrid of those three methods has also been explored
in~\cite{kim2019nvgaze, wang2018hierarchical}.

\paragraph{Image Recognition}

Advanced by the development of deep learning, there have been a notable number
of image recognition models~\cite{simonyan2014very, szegedy2015going,
szegedy2016rethinking, he2016deep, szegedy2017inception} which can achieve
similarly high performance. Among these models, we choose
Inception-v3~\cite{szegedy2016rethinking} for two reasons. First, it has high
computational efficiency and low parameter count with the use of factorization
in convolutions and aggressive regularization, which makes it suitable for uses
in mobile vision and real-time applications. Second, it has been proved that
Inception-v3 can perform well even on low-resolution images for small objects
in~\cite{szegedy2016rethinking}, which is essential in our application.

Inception-v3 is a successor of Inception-v1~\cite{szegedy2015going} which
further improves the computational efficiency of the Inception module. It first
factorizes convolutions with larger spatial filters into smaller convolutions,
for example, replacing one $5 \times 5$ convolution with two layers of $3 \times
3$ convolution. Further, it spatially factorizes convolutions into asymmetric
convolutions by replacing any $n \times n$ convolution by a $1 \times n$
convolution followed by a $n \times 1$ convolution. In addition, it adds tweaks
including regularization with batch-normalized auxiliary classifiers and label
smoothing, which enables training high quality networks using only a modest
sized training set.

\paragraph{Object detection}

Current object detection approaches can be categorized into
one-stage~\cite{redmon2016you, liu2016ssd, lin2017focal} and
two-stage~\cite{girshick2014rich, girshick2015fast, ren2015faster, he2017mask}.
Two-stage detectors first generate several proposals with high probability of
containing an object, and then utilize two sub networks to classify each
proposal and regress their specific positions separately. On the other hand,
one-stage methods detect objects without using a region proposal network (RPN)
for a simpler network architecture and shorter inference time. For more details
please refer to the latest overview\cite{liu2020deep}. However, considering the
deployment in several resource-constrained platforms, such as VR/AR HMD with
memory limitation and high frame rate requirement, both methods are still far
away from being used in practice. To solve this issue, we integrate object
detection with eye tracking to accelerate and reduce network capacity by
decreasing the search space. Some related work~\cite{toyama2012gaze,
ishiguro2010aided, bonino2009blueprint} also combine these two tasks, but they
either do not provide an evaluation of the benefits of using eye tracking, or
require a collection of all exhibits with static backgrounds.

\paragraph{Synthetic eye dataset}

Previous research showed that synthetic eye dataset relieves eyetracking
community from manual data collection and data labeling.
\emph{NVGaze}~\cite{kim2019nvgaze} dataset, for example, renders 2M infrared
images of eyes at 1280 \(\times\) 960 resolution using anatomically-informed eye
and face models.  \emph{SynthesEyes}~\cite{wood2015rendering}, samples 11.4k
photorealistic eye images with a wide range of head poses, gaze directions, and
illumination conditions. As the follow-up work,
\emph{UnityEyes}~\cite{wood2016learning} provides a 200x faster solution of
generating a large number of images.
