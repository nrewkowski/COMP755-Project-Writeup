\section{Related Work}
%nick knows nothing about this. nick doesn't pay attention in class either so
%doesn't know where to look. also neurips, iccv, cvpr, etc. are filled with fake
%papers so nick's not confident in their results anyway

\paragraph{Eye tracking}

Eye tracking is defined as the process of identifying the line of sight for each
eye of a human user at a single instant~\cite{kim2019nvgaze}. In this work, we
focus on vision-based eye tracking, which comprises mainly three methods:
feature-based tracking, model-based tracking, and appearance-based tracking.
Feature-based tracking localizes key eye features, such as the iris center and
eye corner, and maps them to corresponding gaze coordinates on the
screen~\cite{sesma2012evaluation, torricelli2008neural}. Model-based
methods~\cite{wood20163d, wang2017real} estimate eye gaze using a geometrical
model of the human eye to simulate the structure and function of the human
vision system. The space of state-of-the-art appearance-based methods is
occupied by artificial neural networks. A huge collection of eye images are used
to train a neural network to output the final gaze~\cite{zhang2015appearance,
schneider2014manifold, sugano2014learning}. A hybrid of those three methods has
also been explored in~\cite{kim2019nvgaze, wang2018hierarchical}.

\paragraph{Image Recognition}

Image recognition is fundamentally the problem of analysing image data to
recognize a part of the image. Usual tasks include recognizing or classifying
certain objects or their boundaries, including animals, vehicles, and even
textual characters. Example datasets include the MNIST database of handwritten
digits~\cite{lecun_1998} and the COCO (common objects in context)
dataset~\cite{COCO}. One core problem in image recognition tasks is that even
low-resolution images are high-dimensional inputs, since each pixel consists of
3 color channels (red, green, and blue). For a 480p image, or \(640 \times 480\)
pixels, this is \(640 \times 480 \times 3 = 921600\) features per image.
Convolutional neural networks such as in~\citeauthor{lecun_1998}
and~\citeauthor{Krizhevsky_2017} help reduce the impact of this problem with
convolutions and other techniques~\cite{lecun_1998,Krizhevsky_2017}.

Due to advances in deep learning, there have been a notable number of image
recognition models~\cite{simonyan2014very, szegedy2015going,
szegedy2016rethinking, he2016deep, szegedy2017inception} which achieve similar
and strong performance. Among these models, we chose
Inception-v3~\cite{szegedy2016rethinking} for two reasons. First, it has high
computational efficiency and low parameter count with the use of factorization
in convolutions and aggressive regularization, making it suitable for uses
in mobile vision and real-time applications. Second, it has been proven that
Inception-v3 can perform well even on low-resolution images for small objects
in~\cite{szegedy2016rethinking}, which is essential in our application.

Inception-v3 is a successor of Inception-v1~\cite{szegedy2015going} which
further improves the computational efficiency of the Inception module. It first
factorizes convolutions with larger spatial filters into smaller convolutions,
for example, replacing one $5 \times 5$ convolution with two layers of $3 \times
3$ convolution. Further, it spatially factorizes convolutions into asymmetric
convolutions by replacing any $n \times n$ convolution by a $1 \times n$
convolution followed by a $n \times 1$ convolution. In addition, it adds tweaks
including regularization with batch-normalized auxiliary classifiers and label
smoothing, which enables training high quality networks using only a modest
sized training set.

\paragraph{Object detection}

Current object detection approaches can be categorized into one-stage
approaches~\cite{redmon2016you, liu2016ssd, lin2017focal} and two-stage
approaches~\cite{girshick2014rich, girshick2015fast, ren2015faster, he2017mask}.
Two-stage detectors first generate several proposals with high probability of
containing an object, and then use two sub-networks to classify each
proposal and regress their specific positions separately. On the other hand,
one-stage methods detect objects without using a region proposal network (RPN)
for a simpler network architecture and shorter inference time. For more details
please refer to the latest overview\cite{liu2020deep}.

When considering deployment in several resource-constrained platforms, such as
VR/AR HMD with memory limitations and high frame rate requirements, both methods
are still far from practical use. To solve this issue, we integrate object
detection with eye tracking to accelerate performance and reduce network
capacity by decreasing the search space. Some related work~\cite{toyama2012gaze,
ishiguro2010aided, bonino2009blueprint} also combine these two tasks, but they
either do not provide an evaluation of the benefits of using eye tracking or
require a collection of all exhibits with static backgrounds.

\paragraph{Synthetic eye dataset}

Previous research showed that synthetic eye dataset relieves the eye-tracking
research community from manual data collection and data labeling.
\emph{NVGaze}~\cite{kim2019nvgaze} dataset, for example, renders 2 million
infrared images of eyes at \(1280 \times 960\) resolution using
anatomically-informed eye and face models.
\emph{SynthesEyes}~\cite{wood2015rendering} samples 11.4 thousand photorealistic
eye images with a wide range of head poses, gaze directions, and illumination
conditions. As a follow-up work, \emph{UnityEyes}~\cite{wood2016learning}
provides a 200-times faster solution to generating large numbers of images.
